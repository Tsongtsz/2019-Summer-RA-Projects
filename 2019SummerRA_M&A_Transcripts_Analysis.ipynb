{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1: M&A transcripts \n",
    "\n",
    "<b>Creator:</b> Congci(Damon) Hao, conghao@iu.edu\n",
    "\n",
    "<b>Objective:</b> The goal of this project is to show how much mangers know about sources of merger synergies. Based on readings of many cases, Professor Tseng hypothesized that mangers who know more about sources of synergies provide longer answers, more specific answers, and more forward-looking answers. \n",
    "\n",
    "<b>Input data:</b> Each transcript starts with management presentation, but we need only answers in the Q&A session following the presentation. Each M&A transcript is in a separate .txt file with a unique identifier as the file name. \n",
    "\n",
    "\n",
    "<b>Output items:</b>\n",
    "\n",
    "•\tNumber of questions\n",
    "\n",
    "•\tTotal_words\t\n",
    "\n",
    "•\tNumber_Entities\t\n",
    "\n",
    "•\tWords_in_Entities\t\n",
    "\n",
    "•\tNumber_of_Times\t\n",
    "\n",
    "•\tWords_in_Times\t\n",
    "\n",
    "•\tNumber_of_Locations\t\n",
    "\n",
    "•\tWords_in_Locations\t\n",
    "\n",
    "•\tNumber_of_Organizations\t\n",
    "\n",
    "•\tWords_in_Organizations\t\n",
    "\n",
    "•\tNumber_of_Persons\t\n",
    "\n",
    "•\tWords_in_Persons\t\n",
    "\n",
    "•\tNumber_of_Money\t\n",
    "\n",
    "•\tWords_in_Money\t\n",
    "\n",
    "•\tNumber_of_Percentages\t\n",
    "\n",
    "•\tWords_in_Percentages\t\n",
    "\n",
    "•\tNumber_of_Dates\t\n",
    "\n",
    "•\tWords_in_Dates\n",
    "\n",
    "•\tNumber of forward-looking words (Bozanic Roulstone Buskirk 2016 Appendix A word list)\n",
    "\n",
    "•\tNumber of uncertain words (Bozanic et al. 2018 use Loughran and McDonald’s uncertainty measure)\n",
    "\n",
    "•\tNumber of positive words (Harvard dictionary)\n",
    "\n",
    "•\tNumber of negative words (Harvard dictionary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraryies/Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Functions for forward-looking statements extraction.\"\"\"\n",
    "\n",
    "# Standard Library\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import glob\n",
    "import string\n",
    "import argparse\n",
    "\n",
    "# Third Party Libraries\n",
    "import nltk\n",
    "import nltk.data\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Parse XML\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "\n",
    "# Name Entity Recognitation\n",
    "# https://juejin.im/post/5971a4b9f265da6c42353332?utm_source=gold_browser_extension%5D\n",
    "import spacy \n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "# Measure the Readability\n",
    "# https://pypi.org/project/textstat/\n",
    "import textstat\n",
    "\n",
    "# Measure the Sentiment \n",
    "# https://www.analyticsvidhya.com/blog/2018/02/natural-language-processing-for-beginners-using-textblob/\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Regx to select and measure the forward-looking statements\n",
    "LINES = [temp.strip() for temp in open('expressions.txt', 'r').readlines()]\n",
    "REGEX = re.compile(r'%s' % (r'\\b' + r'\\b|\\b'.join(LINES) + r'\\b'),\n",
    "                   re.IGNORECASE)\n",
    "IGNORE = ['call', r'questions?', 'press release', 'slides?', 'webcast',\n",
    "          r'\\?', r'(can|do|will|have) you', r'Q ?:', r'\\[Q', r'\\[?Operator\\]?']\n",
    "REG_IGNORE = re.compile(r'%s' %  r'|'.join(IGNORE), re.IGNORECASE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(text):\n",
    "    \"\"\"Get a list of tokens (words) for a given text.\"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    filtered = [i for i in tokens if not all(j in string.punctuation for j in i)]\n",
    "    final = [w.upper() for w in filtered if not w in stop_words]\n",
    "    \n",
    "    return final\n",
    "\n",
    "# get_tokens(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uncertainty(filein):\n",
    "    \"Count the frequencies of uncertain words from a list stored in uncertainty text file\"\n",
    "    \n",
    "    cnt = Counter()\n",
    "    wanted = re.findall('\\w+',open('uncertainty.txt').read())\n",
    "    text = get_answers(filein)[0].upper()\n",
    "    words = get_tokens(text)\n",
    "    \n",
    "    for word in words:\n",
    "        if word in wanted:\n",
    "            cnt[word] += 1\n",
    "            \n",
    "    total_cnt = sum(dict(cnt).values())\n",
    "    \n",
    "    return total_cnt\n",
    "\n",
    "# get_uncertainty('824351.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Now we will move to page 21.',\n",
       " \"And I'm going to ask Drew to go over the pro forma financial impact.\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_sentences(text):\n",
    "    \"\"\"Sentence tokenizer.\"\"\"\n",
    "    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    return sent_detector.tokenize(text.strip())\n",
    "\n",
    "get_sentences(\"Now we will move to page 21. And I'm going to ask Drew to go over the pro forma financial impact.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_answers(filein):\n",
    "    \"Get the asnwer section of each TEXT disclosure and use it for analyzing the fwd statement, tagging entities\\\n",
    "    , and measuring sentiment\"\n",
    "    CURRENT_DIR = CURRENT_DIR = os.path.dirname(os.path.abspath('__file__'))\n",
    "    folder = os.path.join(CURRENT_DIR, 'atseng_MA_transcript')\n",
    "\n",
    "    path = os.path.join(folder, filein)\n",
    "    data = open(path,'r',encoding='utf-8',errors=\"surrogateescape\")\n",
    "    mess = data.read().replace('\\n',' ')\n",
    "\n",
    "    tokens = word_tokenize(mess)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.remove('a')\n",
    "    stop_words.remove('and')\n",
    "\n",
    "    \n",
    "    mess = [word.upper() for word in tokens if not word in stop_words]\n",
    "    content = ' '.join(mess)\n",
    "\n",
    "    try:\n",
    "        qa = re.search(r'QUESTIONS? ?-?A?N?D? ?-?ANSWERS?(.+)',content).group(1)\n",
    "    except AttributeError:\n",
    "        qa = ''\n",
    "    \n",
    "    qa = get_sentences(qa)\n",
    "    answers = []\n",
    "    questions = 0\n",
    "\n",
    "    \n",
    "    for i in qa:\n",
    "        try:\n",
    "            if i[-2] == '.' or i[-1] == '.':\n",
    "                answers.append(i)\n",
    "            elif \"?\" in i:\n",
    "                questions += 1\n",
    "            else:\n",
    "                question = 0\n",
    "        except IndexError:\n",
    "            continue\n",
    "    \n",
    "    answers = ' '.join(answers)\n",
    "    \n",
    "    return answers, questions\n",
    "#1811892.txt does not have a q&a section\n",
    "# get_answers('1811892.txt')\n",
    "# get_answers(\"3734288.txt\")\n",
    "\n",
    "#Standard output desired from the algorithem \n",
    "# get_answers('1444369.txt')\n",
    "# get_answers('1002821.txt')\n",
    "# get_answers('300327.txt')\n",
    "# get_answers('1080743.txt')\n",
    "\n",
    "#The function can not reach 100% accuracy\n",
    "#This is an example of missclassification that captures questions and answers clauser earlier\n",
    "# get_answers('AAB_882345935_deal_callstreet_8_8_2012.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_fwd(sentence):\n",
    "    \"\"\"Return true if the sentece is a fwd looking statement.\"\"\"\n",
    "    if sentence.isupper():\n",
    "        return False\n",
    "    if REG_IGNORE.search(sentence):\n",
    "        return False\n",
    "    return bool(REGEX.search(sentence))\n",
    "\n",
    "#Example of testing the forward-looking statements\n",
    "is_fwd(\"Now we will move to page 21. And I'm going to ask Drew to go over the pro forma financial impact.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1, 0.5, ['Now we will move to page 21.'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_fwd_statements(text):\n",
    "    \"\"\"Get number of forward-looking statements.\"\"\"\n",
    "    all_sents = get_sentences(text)\n",
    "    len_all = len(all_sents)\n",
    "    if not len_all:\n",
    "        return None, None, None\n",
    "    fwd = 0\n",
    "    fwd_sents = []\n",
    "    for sent in all_sents:\n",
    "        if is_fwd(sent):\n",
    "            fwd += 1\n",
    "            fwd_sents.append(sent)\n",
    "    return len_all, fwd, fwd * 1.0 / len_all, fwd_sents\n",
    "\n",
    "#Extract important measures from forward-looking statements\n",
    "get_fwd_statements(\"Now we will move to page 21. And I'm going to ask Drew to go over the pro forma financial impact.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example: Write a python list into person.csv file\n",
    "\n",
    "import csv\n",
    "csvData = [['Person', 'Age'], ['Peter', '22'], ['Jasmine', '21'], ['Sam', '24']]\n",
    "with open('person.csv', 'w',newline='') as csvFile:\n",
    "    writer = csv.writer(csvFile)\n",
    "    writer.writerows(csvData)\n",
    "csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(answers):\n",
    "    \"Tag all the required entities by spacy\"\n",
    "    \n",
    "    text = nlp(answers)\n",
    "    labels =[x.label_ for x in text.ents]\n",
    "    entities = dict(Counter(labels))\n",
    "    \n",
    "    for entity in ['TIME','LOC','ORG','PERSON','MONEY','PERCENT','DATE']:\n",
    "        if not entity in entities.keys():\n",
    "            entities[entity] = 0\n",
    "    \n",
    "    total_entities = sum(entities.values())\n",
    "    times = entities['TIME']\n",
    "    locations = entities['LOC']\n",
    "    organizations = entities['ORG']\n",
    "    persons = entities['PERSON']\n",
    "    money = entities['MONEY']\n",
    "    percentages = entities['PERCENT']\n",
    "    dates = entities['DATE']\n",
    "        \n",
    "    \n",
    "    return total_entities,times,locations, organizations, persons, money, percentages, dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(answers):\n",
    "    \"Count and sum the number of words in each entity and the number of entities \"\n",
    "    \n",
    "    text = nlp(answers)\n",
    "\n",
    "    labels = set([w.label_ for w in text.ents])\n",
    "    entity_results = dict()\n",
    "    word_results = dict()\n",
    "\n",
    "    for label in labels:\n",
    "        entities = [e.string for e in text.ents if label==e.label_]\n",
    "        \n",
    "        #get the number of words\n",
    "        entity_list = \" \".join(entities).strip()\n",
    "        tokens = word_tokenize(entity_list)\n",
    "        word_results[label] = len(tokens)\n",
    "        \n",
    "        #get the number of entities\n",
    "        entity_results[label] = len(entities)\n",
    "\n",
    "\n",
    "    for cat in ['TIME','LOC','ORG','PERSON','MONEY','PERCENT','DATE']:\n",
    "            if not cat in entity_results.keys():\n",
    "                entity_results[cat] = 0\n",
    "            if not cat in word_results.keys():\n",
    "                word_results[cat] = 0\n",
    "\n",
    "    \n",
    "    \n",
    "    total_entities = sum(entity_results.values())\n",
    "    e_times = entity_results['TIME']\n",
    "    e_locations = entity_results['LOC']\n",
    "    e_organizations = entity_results['ORG']\n",
    "    e_persons = entity_results['PERSON']\n",
    "    e_money = entity_results['MONEY']\n",
    "    e_percentages = entity_results['PERCENT']\n",
    "    e_dates = entity_results['DATE']\n",
    "    \n",
    "    total_entity_words = sum(word_results.values())\n",
    "    w_times = word_results['TIME']\n",
    "    w_locations = word_results['LOC']\n",
    "    w_organizations = word_results['ORG']\n",
    "    w_persons = word_results['PERSON']\n",
    "    w_money = word_results['MONEY']\n",
    "    w_percentages = word_results['PERCENT']\n",
    "    w_dates = word_results['DATE']\n",
    "        \n",
    "    \n",
    "    return total_entities, e_times, e_locations, e_organizations, e_persons, e_money, e_percentages, e_dates,\\\n",
    "    total_entity_words, w_times, w_locations, w_organizations, w_persons, w_money, w_percentages, w_dates,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiments(text):\n",
    "    \"Count the number of positive and negative words based off the LoughranMcDonald_SentimentWordLists_2018\"\n",
    "    \n",
    "    #cnt_pos and cnt_neg store all the instances\n",
    "    cnt_pos = Counter()\n",
    "    cnt_neg = Counter()\n",
    "    words = get_tokens(text)\n",
    "    \n",
    "    wanted_pos = re.findall('\\w+',open('LM_positive.txt').read())\n",
    "    wanted_neg = re.findall('\\w+',open('LM_negative.txt').read())\n",
    "    \n",
    "    for word in words:\n",
    "        if word in wanted_pos:\n",
    "            cnt_pos[word] += 1\n",
    "        elif word in wanted_neg:\n",
    "            cnt_neg[word] += 1\n",
    "            \n",
    "    pos = sum(dict(cnt_pos).values())\n",
    "    neg = sum(dict(cnt_neg).values())\n",
    "    \n",
    "    return pos,neg,cnt_pos,cnt_neg\n",
    "\n",
    "# get_sentiments(get_answers('1080743.txt')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CURRENT_DIR = CURRENT_DIR = os.path.dirname(os.path.abspath('__file__'))\n",
    "folder = os.path.join(CURRENT_DIR, 'atseng_MA_transcript')\n",
    "files = sorted([str(i) for i in os.listdir(folder) if i != '.DS_Store'])\n",
    "\n",
    "#create the header\n",
    "header = [['Id','Number of Questions','Total Words',\n",
    "           'Number of Entities','Words in Entities',\n",
    "           'Number of Times','Words in Times',\n",
    "           'Number of Locations','Words in Locations',\n",
    "           'Number of Organizations','Words in Organizations',\n",
    "           'Number of Persons','Words in Persons',\n",
    "           'Number of Money','Words in Money',\n",
    "           'Number of Percentages','Words in Percentages',\n",
    "           'Number of Dates','Words in Dates',\n",
    "           'Total Sentences', 'Total Forward Sentences', 'Forward Ratio',\n",
    "           'Uncertainty Words',\n",
    "           'Polarity','Subjectivity',\n",
    "           'Positive Words','Negative Words'\n",
    "           ]]\n",
    "\n",
    "#open a new output csv file \n",
    "with open('MA_Transcripts_Analysis.csv', 'w',newline='') as fileout:\n",
    "    writer = csv.writer(fileout)\n",
    "    writer.writerows(header)\n",
    "    print ('\\nScanning %d files in \"%s\"' % (len(files), folder))\n",
    "    \n",
    "    num_txt = 0\n",
    "    no_qa = []\n",
    "    for num, filein in enumerate(files):\n",
    "        basename, extension = os.path.splitext(filein)\n",
    "        \n",
    "        if extension == '.txt':\n",
    "            print(filein)\n",
    "            num_txt += 1\n",
    "            \n",
    "            ID = filein.strip('.txt')\n",
    "\n",
    "            answers,questions = get_answers(filein)\n",
    "           \n",
    "            words = len(get_tokens(answers))\n",
    "\n",
    "            if answers == '':\n",
    "                no_qa.append(filein)\n",
    "                print ('\\tERROR for file %s: 0 length. There is no standard\\\n",
    "                Question and Answers section.' % filein)\n",
    "                continue\n",
    "\n",
    "            total_entities,e_times,e_locations, e_organizations, e_persons, e_money, e_percentages, e_dates,\\\n",
    "            total_entity_words,w_times,w_locations, w_organizations, w_persons, w_money, w_percentages, w_dates,\\\n",
    "            = get_results(answers)\n",
    "\n",
    "            fwd = get_fwd_statements(answers.lower())\n",
    "\n",
    "            #use textblob package to analyze the sentiment\n",
    "            blob = TextBlob(answers)\n",
    "            polarity = blob.sentiment.polarity\n",
    "            subjectivity = blob.sentiment.subjectivity\n",
    "\n",
    "            #count the frequencies of uncertain words \n",
    "            total_cnt = get_uncertainty(filein)\n",
    "            \n",
    "            pos,neg,cnt_pos,cnt_neg = get_sentiments(answers)\n",
    "    #             if verbose:\n",
    "    #                 os.system('clear')\n",
    "    #                 print ('\\n%d)' % num)\n",
    "    #                 for sent in res[3]:\n",
    "    #                     print ('\\t%s' % sent)\n",
    "    #                 _ = raw_input('')\n",
    "\n",
    "            row = [ID, questions, words,\\\n",
    "                total_entities, total_entity_words, e_times, w_times, e_locations, w_locations, \\\n",
    "                e_organizations, w_organizations, e_persons, w_persons, e_money, w_money, \\\n",
    "                e_percentages, w_percentages, e_dates, w_dates,\\\n",
    "                fwd[0], fwd[1], round(fwd[2],2),\\\n",
    "                total_cnt, \\\n",
    "                polarity, subjectivity,\\\n",
    "                pos,neg]\n",
    "\n",
    "            writer.writerow(row)\n",
    "            \n",
    "    print ('\\nAll %d text files are processed.\\nThese %d text files do not have QA sections:\\n' % (num_txt,len(no_qa))) \n",
    "    print(no_qa)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
