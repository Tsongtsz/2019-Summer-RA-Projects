{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Post-crisis Press Releases Analysis\n",
    "\n",
    "<b>Creator:</b> Congci(Damon) Hao, conghao@iu.edu\n",
    "\n",
    "<b>Objective:</b> The goal of this project is to extract content of information in press releases issued following business crises, e.g., oil spills, casualty accidents, or investor class action lawsuits. \n",
    "\n",
    "<b>Text to be processed:</b> Each press release starts a 5-digit company identifier permno, crisis_date, and disclosure_date. Please create three columns for these three. Please follow Marco’s earlier code. Process all text in a press release but remove boilerplate safe harbor statement and business description/contact info.        \n",
    "\n",
    "<b>Output items:</b>\n",
    "\n",
    "•\tReadability_Index\n",
    "\n",
    "•\tTotal_words\t\n",
    "\n",
    "•\tNumber_Entities\t\n",
    "\n",
    "•\tWords_in_Entities\t\n",
    "\n",
    "•\tNumber_of_Times\t\n",
    "\n",
    "•\tWords_in_Times\t\n",
    "\n",
    "•\tNumber_of_Locations\t\n",
    "\n",
    "•\tWords_in_Locations\t\n",
    "\n",
    "•\tNumber_of_Organizations\t\n",
    "\n",
    "•\tWords_in_Organizations\t\n",
    "\n",
    "•\tNumber_of_Persons\t\n",
    "\n",
    "•\tWords_in_Persons\t\n",
    "\n",
    "•\tNumber_of_Money\t\n",
    "\n",
    "•\tWords_in_Money\t\n",
    "\n",
    "•\tNumber_of_Percentages\t\n",
    "\n",
    "•\tWords_in_Percentages\t\n",
    "\n",
    "•\tNumber_of_Dates\t\n",
    "\n",
    "•\tWords_in_Dates\n",
    "\n",
    "•\tNumber of forward-looking words (Bozanic Roulstone Buskirk 2016 Appendix A word list)\n",
    "\n",
    "•\tNumber of uncertain words (Bozanic et al. 2018 use Loughran and McDonald’s uncertainty measure)\n",
    "\n",
    "•\tNumber of positive words (Harvard dictionary)\n",
    "\n",
    "•\tNumber of negative words (Harvard dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import time\n",
    "import codecs\n",
    "import string\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Third Party Libraries\n",
    "import nltk\n",
    "import nltk.data\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import opinion_lexicon\n",
    "\n",
    "# Name Entity Recognitation\n",
    "# https://juejin.im/post/5971a4b9f265da6c42353332?utm_source=gold_browser_extension%5D\n",
    "import spacy \n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "# Measure the Readability\n",
    "# https://pypi.org/project/textstat/\n",
    "import textstat\n",
    "\n",
    "# Measure the Sentiment \n",
    "# https://www.analyticsvidhya.com/blog/2018/02/natural-language-processing-for-beginners-using-textblob/\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run all pre-requisites\n",
    "\n",
    "REGEX = r'\\d{5} \\d+/\\d+/\\d+ \\d+/\\d+/\\d+'\n",
    "TAG = r'<(.*?)>(.*?)</(.*?)>'\n",
    "\n",
    "LINES = [temp.strip() for temp in open('expressions.txt', 'r').readlines()]\n",
    "FWD_REGEX = re.compile(r'%s' % (r'\\b' + r'\\b|\\b'.join(LINES) + r'\\b'),\n",
    "                   re.IGNORECASE)\n",
    "IGNORE = ['call', r'questions?', 'press release', 'slides?', 'webcast',\n",
    "          r'\\?', r'(can|do|will|have) you', r'Q ?:', r'\\[Q', r'\\[?Operator\\]?']\n",
    "REG_IGNORE = re.compile(r'%s' %  r'|'.join(IGNORE), re.IGNORECASE)\n",
    "\n",
    "CURRENT_DIR = os.path.dirname(os.path.abspath('__file__'))\n",
    "FILE = '218_disclosures.txt'\n",
    "PATH = os.path.join(CURRENT_DIR, FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example: Write a python list into person.csv file\n",
    "\n",
    "csvData = [['Person', 'Age'], ['Peter', '22'], ['Jasmine', '21'], ['Sam', '24']]\n",
    "with open('person.csv', 'w',newline='') as csvFile:\n",
    "    writer = csv.writer(csvFile)\n",
    "    writer.writerows(csvData)\n",
    "csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(text):\n",
    "    \"\"\"Get a list of tokens (words) for a given text.\"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    filtered = [i for i in tokens if not all(j in string.punctuation for j in i)]\n",
    "    final = [w.upper() for w in filtered if not w in stop_words]\n",
    "    \n",
    "    return final\n",
    "\n",
    "# get_tokens(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_splitter(data, headline=True):\n",
    "    \"\"\"Generator of article chunks.\"\"\"\n",
    "    buff = []\n",
    "    for line in data:\n",
    "        if re.findall(REGEX, line):\n",
    "            if buff:\n",
    "                if not headline:\n",
    "                    buff.pop(1)\n",
    "                yield u' '.join(buff)\n",
    "                buff[:] = []\n",
    "        if line.strip():\n",
    "            buff.append(line.strip())\n",
    "    yield u' '.join(buff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_articles(headline=True):\n",
    "    \"\"\"Split the document file into articles.\"\"\"\n",
    "    path = os.path.join(CURRENT_DIR, '218_disclosures.txt')\n",
    "    lines = codecs.open(path, 'rU', 'latin').readlines()\n",
    "    docs = list(dump_splitter(lines, headline=headline))\n",
    "    return docs\n",
    "\n",
    "# get_articles(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('548270302200503212005', 21)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_text(doc):\n",
    "    \"\"\"Preprocess text.\"\"\"\n",
    "    # Extract preamble\n",
    "    preamble = re.findall(REGEX, doc)[0].split()\n",
    "    \n",
    "    month_c = preamble[1].split('/')[0].zfill(2)\n",
    "    day_c = preamble[1].split('/')[1].zfill(2)\n",
    "    year_c = preamble[1].split('/')[2]\n",
    "    crsis_date = month_c+day_c+year_c\n",
    "    \n",
    "    month_d = preamble[2].split('/')[0].zfill(2)\n",
    "    day_d = preamble[2].split('/')[1].zfill(2)\n",
    "    year_d = preamble[2].split('/')[2]\n",
    "    disclosure_date = month_d+day_d+year_d\n",
    "    \n",
    "    #create an identifier to match with the bog_identifiers\n",
    "    identifier = preamble[0]+crsis_date+disclosure_date\n",
    "#     identifier = [preamble[0],\n",
    "#             datetime.strptime(preamble[1],'%m/%d/%Y'),\n",
    "#             datetime.strptime(preamble[2],'%m/%d/%Y')]\n",
    "#     identifier =\"{:02}\".format(preamble[2])\n",
    "#     identifier = preamble[2]\n",
    "    \n",
    "    text = re.sub(REGEX, '', doc).strip()\n",
    "       \n",
    "    # Remove irrelevant text\n",
    "    text = re.sub((r'. (More information|For information on|For more '\n",
    "                   'information) .*?$'), '', text)\n",
    "\n",
    "    # Titlecase uppercase headlines\n",
    "    capital = ''\n",
    "    for char in text:\n",
    "        if char.isupper() or char in string.punctuation + ' ':\n",
    "            capital += char\n",
    "        else:\n",
    "            break\n",
    "    if len(capital.split()) > 3:\n",
    "        text = text.replace(capital, capital.title())\n",
    "\n",
    "    return text, preamble,identifier\n",
    "\n",
    "text, preamble,identifier = preprocess_text(get_articles(PATH)[100])\n",
    "identifier,len(identifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22.75, 13.22)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_readability(text):\n",
    "    \"Calculate some readability measures from the textstat package \"\n",
    "    \n",
    "    #Return the Flesch Reading Ease Score\n",
    "    read_ease = textstat.flesch_reading_ease(text)\n",
    "    \n",
    "    #Return the Fog Index Grade\n",
    "    read_grade = textstat.gunning_fog(text)\n",
    "    \n",
    "    return read_ease,read_grade\n",
    "\n",
    "get_readability(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_fwd(sentence):\n",
    "    \"\"\"Return true if the sentece is a fwd looking statement.\"\"\"\n",
    "    if sentence.isupper():\n",
    "        return False\n",
    "    if REG_IGNORE.search(sentence):\n",
    "        return False\n",
    "    return bool(FWD_REGEX.search(sentence))\n",
    "\n",
    "is_fwd(\"Now we will move to page 21. And I'm going to ask Drew to go over the pro forma financial impact.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Now we will move to page 21.',\n",
       " \"And I'm going to ask Drew to go over the pro forma financial impact.\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_sentences(text):\n",
    "    \"\"\"Sentence tokenizer.\"\"\"\n",
    "    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    return sent_detector.tokenize(text.strip())\n",
    "\n",
    "get_sentences(\"Now we will move to page 21. And I'm going to ask Drew to go over the pro forma financial impact.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1, 0.5, ['Now we will move to page 21.'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_fwd_statements(text):\n",
    "    \"\"\"Get number of forward-looking statements.\"\"\"\n",
    "    all_sents = get_sentences(text)\n",
    "    len_all = len(all_sents)\n",
    "    if not len_all:\n",
    "        return None, None, None\n",
    "    fwd = 0\n",
    "    fwd_sents = []\n",
    "    for sent in all_sents:\n",
    "        if is_fwd(sent):\n",
    "            fwd += 1\n",
    "            fwd_sents.append(sent)\n",
    "    return len_all, fwd, fwd * 1.0 / len_all, fwd_sents\n",
    "\n",
    "get_fwd_statements(\"Now we will move to page 21. And I'm going to ask Drew to go over the pro forma financial impact.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(text):\n",
    "    \"Count and sum the number of words in each entity and the number of entities \"\n",
    "    \n",
    "    text = nlp(text)\n",
    "\n",
    "    labels = set([w.label_ for w in text.ents])\n",
    "    entity_results = dict()\n",
    "    word_results = dict()\n",
    "\n",
    "    for label in labels:\n",
    "        entities = [e.string for e in text.ents if label==e.label_]\n",
    "        \n",
    "        #get the number of words\n",
    "        entity_list = \" \".join(entities).strip()\n",
    "        tokens = word_tokenize(entity_list)\n",
    "        word_results[label] = len(tokens)\n",
    "        \n",
    "        #get the number of entities\n",
    "        entity_results[label] = len(entities)\n",
    "\n",
    "\n",
    "    for cat in ['TIME','LOC','ORG','PERSON','MONEY','PERCENT','DATE']:\n",
    "            if not cat in entity_results.keys():\n",
    "                entity_results[cat] = 0\n",
    "            if not cat in word_results.keys():\n",
    "                word_results[cat] = 0\n",
    "\n",
    "    \n",
    "    \n",
    "    total_entities = sum(entity_results.values())\n",
    "    e_times = entity_results['TIME']\n",
    "    e_locations = entity_results['LOC']\n",
    "    e_organizations = entity_results['ORG']\n",
    "    e_persons = entity_results['PERSON']\n",
    "    e_money = entity_results['MONEY']\n",
    "    e_percentages = entity_results['PERCENT']\n",
    "    e_dates = entity_results['DATE']\n",
    "    \n",
    "    total_entity_words = sum(word_results.values())\n",
    "    w_times = word_results['TIME']\n",
    "    w_locations = word_results['LOC']\n",
    "    w_organizations = word_results['ORG']\n",
    "    w_persons = word_results['PERSON']\n",
    "    w_money = word_results['MONEY']\n",
    "    w_percentages = word_results['PERCENT']\n",
    "    w_dates = word_results['DATE']\n",
    "        \n",
    "    \n",
    "    return total_entities, e_times, e_locations, e_organizations, e_persons, e_money, e_percentages, e_dates,\\\n",
    "    total_entity_words, w_times, w_locations, w_organizations, w_persons, w_money, w_percentages, w_dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, Counter({'UNSPECIFIED': 1, 'BELIEVES': 1}))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_uncertainty(text):\n",
    "    \"Count the frequencies of uncertain words from a list stored in uncertainty text file\"\n",
    "    \n",
    "    cnt = Counter()\n",
    "    wanted = re.findall('\\w+',open('LM_uncertainty.txt').read())\n",
    "    words = get_tokens(text)\n",
    "    \n",
    "    for word in words:\n",
    "        if word in wanted:\n",
    "            cnt[word] += 1\n",
    "            \n",
    "    total_cnt = sum(dict(cnt).values())\n",
    "    \n",
    "    return total_cnt,cnt\n",
    "\n",
    "get_uncertainty(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiments(text):\n",
    "    \"Count the number of positive and negative words based off the LoughranMcDonald_SentimentWordLists_2018\"\n",
    "  \n",
    "    cnt_pos = Counter()\n",
    "    cnt_neg = Counter()\n",
    "    words = get_tokens(text)\n",
    "    \n",
    "    wanted_pos = re.findall('\\w+',open('LM_positive.txt').read())\n",
    "    wanted_neg = re.findall('\\w+',open('LM_negative.txt').read())\n",
    "    \n",
    "    for word in words:\n",
    "        if word in wanted_pos:\n",
    "            cnt_pos[word] += 1\n",
    "        elif word in wanted_neg:\n",
    "            cnt_neg[word] += 1\n",
    "            \n",
    "    pos = sum(dict(cnt_pos).values())\n",
    "    neg = sum(dict(cnt_neg).values())\n",
    "    \n",
    "    return pos,neg,cnt_pos,cnt_neg\n",
    "\n",
    "# get_sentiments(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Bog Index By StyleWriter\n",
    "\n",
    "Check the other script,\"Python Automation With pyautogui\", automating the analysis of each disclosure in StyleWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\47523\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "#Retrieve Bog Index from \"SW4stats.txt\"\n",
    "\n",
    "BOG = pd.read_csv(\"SW4stats.txt\",delimiter=\"\\\\t\")\n",
    "\n",
    "# Need to drop the irrelevant Time column to check the duplicates of all other columns\n",
    "BOG = BOG.drop(['Date & Time'], axis=1)\n",
    "\n",
    "#Check the duplicates\n",
    "# bog_index[bog_index['Document']=='93080_08112010_09012010.docx']\n",
    "BOG = BOG.drop_duplicates(subset='Document',keep='first')\n",
    "\n",
    "l = [re.sub('[^0-9]+','',''.join(i))[:21] for i in list(BOG['Document'].str.split('_'))]\n",
    "    \n",
    "BOG['ID'] = l\n",
    "BOG = BOG.set_index(['ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Final Script Combining All Pre-Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scanning 363 articles in \"218_disclosures.txt\" \n",
      "\n",
      "218 files are confirmed and ready to be processed \n",
      "\n",
      "All done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### RUN THIS ###\n",
    "\n",
    "#create the header\n",
    "header = [['Id','Crsis_Date','Disclosure_Date', 'Total Words',\n",
    "           'Bog Index','Flesch Reading Ease Score','Gunning Fog Index',\n",
    "           'Number of Entities','Words in Entities',\n",
    "           'Number of Times','Words in Times',\n",
    "           'Number of Locations','Words in Locations',\n",
    "           'Number of Organizations','Words in Organizations',\n",
    "           'Number of Persons','Words in Persons',\n",
    "           'Number of Money','Words in Money',\n",
    "           'Number of Percentages','Words in Percentages',\n",
    "           'Number of Dates','Words in Dates',\n",
    "           'Total Sentences', 'Total Forward Sentences', 'Forward Ratio',\n",
    "           'Uncertainty Words',\n",
    "           'Polarity','Subjectivity',\n",
    "           'Positive Words','Negative Words'\n",
    "           ]]\n",
    "\n",
    "#open a new output csv file \n",
    "with open('output.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerows(header)\n",
    "    \n",
    "    #parse document into list of articles\n",
    "    docs = get_articles(PATH)       \n",
    "    print ('\\nScanning %d articles in \"%s\" \\n' % (len(docs), FILE))\n",
    "    \n",
    "    print('%d files are confirmed and ready to be processed \\n' % len(BOG.index))\n",
    "    \n",
    "    for num, doc in enumerate(docs):\n",
    "        \n",
    "        text, preamble,identifier = preprocess_text(doc)\n",
    "#         print('Article: ' + preamble[0] + '\\n')\n",
    "                \n",
    "        ID = preamble[0]\n",
    "        crsis_date = preamble[1]\n",
    "        disclosure_date = preamble[2]\n",
    "        \n",
    "        if text == None:\n",
    "            print ('\\tERROR for file %s: 0 length' % ID)\n",
    "            continue\n",
    "            \n",
    "        if identifier in BOG.index:\n",
    "            \n",
    "            total_entities, e_times, e_locations, e_organizations, e_persons, e_money, e_percentages, e_dates,\\\n",
    "            total_entity_words, w_times, w_locations, w_organizations, w_persons, w_money, w_percentages, w_dates \\\n",
    "            = get_results(text)\n",
    "\n",
    "            bog_index = BOG.loc[identifier]['Bog Index']\n",
    "            \n",
    "            read_ease,read_grade = get_readability(text)\n",
    "\n",
    "            total_words = len(get_tokens(text))\n",
    "\n",
    "            fwd = get_fwd_statements(text)\n",
    "\n",
    "            #count the frequencies of uncertain words \n",
    "            total_cnt,cnt = get_uncertainty(text)\n",
    "\n",
    "            #use textblob package to analyze the sentiment\n",
    "            blob = TextBlob(text)\n",
    "            polarity = round(blob.sentiment.polarity,2)\n",
    "            subjectivity = round(blob.sentiment.subjectivity,2)\n",
    "\n",
    "            #count the positive and negative words\n",
    "            pos, neg, cnt_pos, cnt_neg = get_sentiments(text)\n",
    "\n",
    "            row = [ID,crsis_date, disclosure_date, total_words,\\\n",
    "                    bog_index,read_ease,read_grade,\\\n",
    "                    total_entities, total_entity_words, e_times, w_times, e_locations, w_locations, \\\n",
    "                    e_organizations, w_organizations, e_persons, w_persons, e_money, w_money, \\\n",
    "                    e_percentages, w_percentages, e_dates, w_dates,\\\n",
    "                    fwd[0],fwd[1],round(fwd[2],2),\\\n",
    "                    total_cnt,\\\n",
    "                    polarity, subjectivity,\\\n",
    "                    pos,neg] \n",
    "\n",
    "\n",
    "            writer.writerow(row)\n",
    "        \n",
    "        #list the positive and negative words by documents\n",
    "#         print('Positive Words： ' + str(cnt_pos) + '\\nNegative Words: ' + str(cnt_neg) + '\\nUncertainty Words: ' + str(cnt) + '\\n\\n')\n",
    "        \n",
    "        else:      \n",
    "            continue\n",
    "            \n",
    "    print('All done.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
